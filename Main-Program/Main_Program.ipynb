{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Confirmation-Bias-Analyser/Main-Program/blob/main/Main%20Program%20(Notebook_Version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWfyZWp7rj7E"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers\n",
        "!pip install vaderSentiment\n",
        "!pip install anytree\n",
        "!pip install dash\n",
        "!pip install jupyter-dash\n",
        "!pip install pyvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doYpuVELNMSi"
      },
      "source": [
        "# Import essential libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJjKKLkkW8Bg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import random\n",
        "import re\n",
        "\n",
        "# The shutil module offers a number of high-level operations on files and collections of files.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import sys\n",
        "mainDirectory = \"/content/drive/MyDrive/FYP/Analyser/\"\n",
        "sys.path.append(mainDirectory + 'Functions/')\n",
        "from confirmation_bias_model_functions import *\n",
        "from data_collection_functions import *\n",
        "from data_preprocessing_functions import *\n",
        "from visualisation_functions import *\n",
        "from verification_functions import *\n",
        "\n",
        "# from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "\n",
        "from jupyter_dash import JupyterDash\n",
        "from dash import Dash, html, Input, Output, dash_table, dcc\n",
        "from dash.dependencies import Input, Output\n",
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8OgZp1eJcjx"
      },
      "source": [
        "# Authenticate API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1SApui7aOAQ"
      },
      "outputs": [],
      "source": [
        "with open(mainDirectory + 'twitter_bearer_token.txt', 'r', encoding=\"utf8\") as f:\n",
        "    token = f.read()\n",
        "\n",
        "header = create_Twitter_headers(token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "BGK3AGIY8LUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# Data Processing #############\n",
        "\n",
        "from anytree import Node, RenderTree, search\n",
        "import re\n",
        "import urllib\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "\n",
        "def getTweetComments(conversation_data):\n",
        "    conversation_dict = {'id':[], 'timestamp':[], 'reply_to':[], 'comment':[]}\n",
        "\n",
        "    for i in conversation_data:\n",
        "        print('User ID:', i['id'], \n",
        "              'Time:', i['user']['created_at'])\n",
        "        print('In reply to:', i['in_reply_to_status_id'])\n",
        "        print(i['text'], '\\n')\n",
        "\n",
        "        conversation_dict['id'].append(i['id'])\n",
        "        conversation_dict['timestamp'].append(i['user']['created_at'])\n",
        "        conversation_dict['reply_to'].append(i['in_reply_to_status_id'])\n",
        "        conversation_dict['comment'].append(i['text'])\n",
        "\n",
        "    return conversation_dict\n",
        "\n",
        "def getLinks(string):\n",
        "    urls = re.findall(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", string)\n",
        "    links = ''\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            opener = urllib.request.build_opener()\n",
        "            request = urllib.request.Request(url)\n",
        "            response = opener.open(request)\n",
        "            actual_url = response.geturl()\n",
        "                      \n",
        "            if '](' in actual_url:\n",
        "                actual_url = actual_url.split('](')[0]\n",
        "          \n",
        "            links += actual_url + ';'\n",
        "            \n",
        "            \n",
        "        except:\n",
        "            if '](' in url:\n",
        "                url = url.split('](')[0]\n",
        "          \n",
        "            links += url + ';'\n",
        "\n",
        "    return links\n",
        "\n",
        "def getURLfromList(url):\n",
        "    if ';' in url:\n",
        "        url = url.split(';')[:-1]\n",
        "        result = []\n",
        "    \n",
        "        for i in url:\n",
        "            result.append(urlparse(i).hostname)\n",
        "        \n",
        "        return result\n",
        "\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def printDetailsPHEME(threads, data):\n",
        "    rumours = 0\n",
        "    non_rumours = 0\n",
        "\n",
        "    for i in threads:\n",
        "        path = '/content/all-rnr-annotated-threads/' + i\n",
        "        print(i)\n",
        "\n",
        "        for j in os.listdir(path):\n",
        "          \n",
        "            for k in data:\n",
        "\n",
        "                for l in os.listdir(path + '/' + k):\n",
        "                    if k == data[0] and l[0] != '.':\n",
        "                        non_rumours += 1\n",
        "\n",
        "                    elif k == data[1] and l[0] != '.':\n",
        "                        rumours += 1\n",
        "\n",
        "    print('Rumours:', rumours)\n",
        "    print('Non-rumours:', non_rumours)\n",
        "    print()\n",
        "\t\n",
        "def traceConversation(dataframe, tree, node, printGraphOption = True):\n",
        "    children_nodes_list = getAllChildNodes(tree, node, [])\n",
        "\n",
        "    print('\\n\\n')\n",
        "    new = search.find_by_attr(tree, node)\n",
        "    \n",
        "    if printGraphOption:\n",
        "        printGraph(new)\n",
        "\n",
        "    return dataframe[(dataframe['reply_to'].isin(children_nodes_list)) | (dataframe['id'].isin(children_nodes_list + [node]))], new\n",
        "\n",
        "def getAllChildNodes(tree, node, children_nodes_list):\n",
        "    children_nodes = search.find_by_attr(tree, node).children\n",
        "    \n",
        "    for i in children_nodes:\n",
        "        children_nodes_list.append(i.name)\n",
        "        \n",
        "        if i.children != None:\n",
        "            getAllChildNodes(tree, i.name, children_nodes_list)\n",
        "            \n",
        "        else:\n",
        "            return\n",
        "            \n",
        "    return children_nodes_list\n",
        "\n",
        "def cleanComments(comments_array):\n",
        "    sentences = []\n",
        "\n",
        "    for i in comments_array:\n",
        "        sequence = i.replace('\\n', ' ') # Remove new line characters\n",
        "        sequence = sequence.replace('\\.', '')\n",
        "        sequence = sequence.replace('.', '')\n",
        "        sequence = sequence.replace(\",\", \" \")\n",
        "        sequence = sequence.replace(\"'\", \" \")\n",
        "        sequence = sequence.replace('\\\\', '')\n",
        "        sequence = sequence.replace('\\'s', '')\n",
        "        sequence = sequence.replace('&gt;', '') # Remove ampersand\n",
        "        sequence = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", sequence) # Remove the user name\n",
        "        sentences.append(sequence)\n",
        "\n",
        "    return sentences\n",
        "    \n",
        "def createTweetsTree(dictionary, tree_root):\n",
        "    for key, item in dictionary.items():\n",
        "        child = Node(key, parent=tree_root)\n",
        "\n",
        "        if len(dictionary[key]) != 0:      \n",
        "            createTweetsTree(dictionary[key], child)\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "def make_map(list_child_parent):\n",
        "    has_parent = set()\n",
        "    all_items = {}\n",
        "    \n",
        "    for child, parent in list_child_parent:\n",
        "        if parent not in all_items:\n",
        "            all_items[parent] = {}\n",
        "            \n",
        "        if child not in all_items:\n",
        "            all_items[child] = {}\n",
        "        \n",
        "        all_items[parent][child] = all_items[child]\n",
        "        has_parent.add(child)\n",
        "\n",
        "    result = {}\n",
        "    \n",
        "    for key, value in all_items.items():\n",
        "        if key not in has_parent:\n",
        "            result[key] = value\n",
        "    \n",
        "    return result\n",
        "\n",
        "def printGraph(root):\n",
        "    for pre, fill, node in RenderTree(root):\n",
        "        print(\"%s%s\" % (pre, node.name))    "
      ],
      "metadata": {
        "id": "_gUUQhiB8SyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Results Verification #############\n",
        "\n",
        "# from Functions.data_collection_functions import *\n",
        "# from Functions.confirmation_bias_model_functions import *\n",
        "import re\n",
        "\n",
        "# def obtainTweetsAndLikes(userID, header):\n",
        "#     selectedUserTweets = getTweetsByUserID(userID, header, 100)\n",
        "#     userLikedTweets = getTweetsLikedByUser(userID, header, 100)\n",
        "#     allTweets = combineTweets([selectedUserTweets, userLikedTweets])\n",
        "\n",
        "#     return allTweets\n",
        "\n",
        "def combineTweets(listOfTweets):\n",
        "    allTweets_ = []\n",
        "\n",
        "    for i in listOfTweets:\n",
        "        for j in i['data']:\n",
        "            allTweets_.append(j['text'])\n",
        "\n",
        "    return allTweets_\n",
        "\n",
        "def checkForRepliesToNews(textList):\n",
        "    result = []\n",
        "    for i in textList:\n",
        "        reply = re.findall(\"(@[A-Za-z0-9]+)\", i)\n",
        "\n",
        "        if any(x in ['@MothershipSG', '@straits_times', '@ChannelNewsAsia', '@YahooSG'] for x in reply):\n",
        "            result.append(i)\n",
        "\n",
        "    return result\n",
        "\n",
        "def calculateUserBias(tweetsData, embedder, defaultClusterSize = 3):\n",
        "    user_tweets = []\n",
        "    textblob_polarity_res = []\n",
        "    textblob_subjectivity_res = [] \n",
        "    vader_compound_scores = []\n",
        "    model_subjectivity_score = []\n",
        "    clean_text = []\n",
        "\n",
        "    for i in tweetsData:\n",
        "        # repliedAccounts = re.findall(\"(@[A-Za-z0-9]+)\", i)\n",
        "\n",
        "        # if any(x in ['@MothershipSG', '@straits_times', '@ChannelNewsAsia'] for x in repliedAccounts):\n",
        "        user_tweets.append(i)\n",
        "        reply = cleanComments([i])\n",
        "        clean_text.append(reply[0])\n",
        "        sentimentalResults = getSentimentalResults(reply[0])\n",
        "\n",
        "        textblob_polarity_res.append(sentimentalResults['textblob_polarity'])\n",
        "        textblob_subjectivity_res.append(sentimentalResults['textblob_subjectivity'])\n",
        "        vader_compound_scores.append(sentimentalResults['vader_compound_scores'])\n",
        "        model_subjectivity_score.append(float(predictFromModel(model, tokenizer, reply)))\n",
        "\n",
        "    overall_subjectivity = []\n",
        "    for i in range(len(model_subjectivity_score)):\n",
        "        overall_subjectivity.append(defineSubjectivity(model_subjectivity_score[i], textblob_subjectivity_res[i]))\n",
        "\n",
        "    overall_polarity = []\n",
        "    for i in range(len(textblob_polarity_res)):\n",
        "        overall_polarity.append(definePolarity(textblob_polarity_res[i], vader_compound_scores[i]))\n",
        "\n",
        "    cluster = getClusters(clean_text, embedder, defaultClusterSize)\n",
        "\n",
        "    df = pd.DataFrame(list(zip(user_tweets, textblob_polarity_res, textblob_subjectivity_res, vader_compound_scores, model_subjectivity_score, overall_subjectivity, overall_polarity, cluster)),\n",
        "                      columns =['tweet', 'textblob_polarity', 'textblob_subjectivity', 'vader_compound_score', 'model_subjectivity', 'overall_subjectivity', 'overall_polarity', 'topic_cluster'])\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "H7l1VC058X3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Data Visualisation #############\n",
        "\n",
        "from anytree import Node, RenderTree, search\n",
        "import networkx as nx\n",
        "import requests\n",
        "from pyvis.network import Network\n",
        "\n",
        "def createNetworkGraph(conversation_tree, head_thread):\n",
        "    G = nx.Graph()\n",
        "    G.add_node(conversation_tree.name)\n",
        "\n",
        "    for _, __, node in RenderTree(conversation_tree):\n",
        "    \n",
        "        try:\n",
        "            G.add_edge(node.parent.name, node.name)\n",
        "\n",
        "        except:\n",
        "            if node.name == head_thread:\n",
        "                continue\n",
        "\n",
        "            G.add_edge(head_thread, node.name)\n",
        "\n",
        "    return G\n",
        "    \n",
        "def createInterativeNetworkGraph(conversation_tree, head_thread, map_dict, scoreDict):\n",
        "    G = Network(\"500px\", \"500px\", notebook=True)\n",
        "    \n",
        "    if len(scoreDict) > 0:\n",
        "        option = True\n",
        "    else:\n",
        "        option = False\n",
        "\n",
        "    for _, __, node in RenderTree(conversation_tree):\n",
        "        try:\n",
        "            if option:\n",
        "                G.add_node(node.name, label=node.name, color=map_dict[node.name], title='Score: ' + str(scoreDict[node.name]))\n",
        "                \n",
        "            else:\n",
        "                G.add_node(node.name, label=node.name, color=map_dict[node.name])\n",
        "\n",
        "        except:\n",
        "            G.add_node(node.name, label=node.name)\n",
        "\n",
        "    for _, __, node in RenderTree(conversation_tree):\n",
        "    \n",
        "        try:\n",
        "            G.add_edge(node.parent.name, node.name)\n",
        "\n",
        "        except:\n",
        "            if node.name == head_thread:\n",
        "                continue\n",
        "\n",
        "            G.add_edge(head_thread, node.name)\n",
        "\n",
        "    return G\n",
        "    \n",
        "def getColourNodes(conversationDF):\n",
        "    polarity_map = {}\n",
        "    subjectivity_map = {}\n",
        "    potential_bias_map = {}\n",
        "\n",
        "    for i in conversationDF.index.tolist():\n",
        "        \n",
        "        ########## Polarity detection results\n",
        "        \n",
        "        if conversationDF['overall_polarity'].loc[i] == 'POS':\n",
        "            polarity_map[conversationDF['id'].loc[i]] = '#00FF80' # Green colour\n",
        "\n",
        "        elif conversationDF['overall_polarity'].loc[i] == 'NEG':\n",
        "            polarity_map[conversationDF['id'].loc[i]] = '#FF9999' # Light pink colour\n",
        "        \n",
        "        # Neutral class or Unknown\n",
        "        else:\n",
        "            polarity_map[conversationDF['id'].loc[i]] = '#FFFF00' # Yellow colour\n",
        "   \n",
        "        ########## Subjectivity detection results\n",
        "        \n",
        "        if conversationDF['overall_subjectivity'].loc[i] == 'SUBJECTIVE':\n",
        "            subjectivity_map[conversationDF['id'].loc[i]] = '#A9A9A9' # Light grey colour\n",
        "\n",
        "        elif conversationDF['overall_subjectivity'].loc[i] == 'OBJECTIVE':\n",
        "            subjectivity_map[conversationDF['id'].loc[i]] = '#ADD8E6' # Light blue colour\n",
        "           \n",
        "        # Unknown class\n",
        "        else:\n",
        "            subjectivity_map[conversationDF['id'].loc[i]] = '#FF7F50' # Red orange colour\n",
        "            \n",
        "        ########## Potential Bias\n",
        "        \n",
        "        if conversationDF['potential_bias'].loc[i] == 1:\n",
        "            potential_bias_map[conversationDF['id'].loc[i]] = 'red'\n",
        "\n",
        "        elif conversationDF['potential_bias'].loc[i] == 0:\n",
        "            potential_bias_map[conversationDF['id'].loc[i]] = 'black'\n",
        "            \n",
        "    return polarity_map, subjectivity_map, potential_bias_map"
      ],
      "metadata": {
        "id": "TuFu69Yg8b6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Confirmation Bias Model #############\n",
        "\n",
        "from textblob import TextBlob\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\t\n",
        "def calculateBias(dataset):\n",
        "    count_positive_polarity_supportive = 0\n",
        "    count_negative_polarity_supportive = 0\n",
        "    count_positive_polarity_unsupportive = 0\n",
        "    count_negative_polarity_unsupportive = 0\n",
        "\n",
        "    for i in dataset.index.tolist():\n",
        "        if dataset['vader_compound_score'].loc[i] > 0.35 and dataset['topic_cluster'].loc[i] == 1:\n",
        "            count_positive_polarity_supportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] < -0.35 and dataset['topic_cluster'].loc[i] == 1:\n",
        "            count_negative_polarity_supportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] > 0.35 and dataset['topic_cluster'].loc[i] == 0:\n",
        "            count_positive_polarity_unsupportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] < -0.35 and dataset['topic_cluster'].loc[i] == 0:\n",
        "            count_negative_polarity_unsupportive += 1\n",
        "            \n",
        "    total = count_positive_polarity_supportive + count_negative_polarity_supportive + count_positive_polarity_unsupportive + count_negative_polarity_unsupportive\n",
        "    \n",
        "    prob_D = (count_positive_polarity_supportive + count_negative_polarity_supportive)/total\n",
        "    prob_D_prime = (count_positive_polarity_unsupportive + count_negative_polarity_unsupportive)/total\n",
        "    result = {'P(D)': prob_D, 'P(D_p)': prob_D_prime}\n",
        "\n",
        "    try:\n",
        "        prob_D_H = count_positive_polarity_supportive / (count_positive_polarity_supportive + count_positive_polarity_unsupportive)\n",
        "        prob_D_Hprime = count_negative_polarity_supportive / (count_negative_polarity_supportive + count_negative_polarity_unsupportive)\n",
        "        \n",
        "        if prob_D_H/prob_D_Hprime > 1:\n",
        "            final_result = 1 / (prob_D_H/prob_D_Hprime)\n",
        "            \n",
        "        else:\n",
        "            final_result = prob_D_H/prob_D_Hprime\n",
        "\n",
        "        # return prob_D_H, prob_D_Hprime, final_result\n",
        "        return final_result\n",
        "\n",
        "    except:\n",
        "        prob_D_H  = 0\n",
        "        prob_D_Hprime = 0\n",
        "\n",
        "        # return prob_D_H, prob_D_Hprime, 1\n",
        "        return 1\n",
        "        \n",
        "def getClusters(allSentences, embedder, num_clusters = 2):\n",
        "    corpus_embeddings = embedder.encode(allSentences)\n",
        "\n",
        "    # Perform kmean clustering\n",
        "    clustering_model = KMeans(n_clusters=num_clusters)\n",
        "    clustering_model.fit(corpus_embeddings)\n",
        "    cluster_assignment = clustering_model.labels_\n",
        "\n",
        "    clustered_sentences = [[] for i in range(num_clusters)]\n",
        "    \n",
        "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "        clustered_sentences[cluster_id].append([allSentences[sentence_id], sentence_id])\n",
        "\n",
        "    return cluster_assignment        \n",
        "\n",
        "def getSentimentalResults(sentence, vaderObject = sid_obj):\n",
        "    textBlobResult = TextBlob(sentence)\n",
        "    vaderResult = vaderObject.polarity_scores(sentence)\n",
        "    compoundScore = vaderResult.pop('compound')\n",
        "    \n",
        "    overallResult = {'textblob_polarity': textBlobResult.sentiment.polarity, \n",
        "                     'textblob_subjectivity': textBlobResult.sentiment.subjectivity,\n",
        "                     'vader_results': vaderResult,\n",
        "                     'vader_compound_scores': compoundScore}\n",
        "\n",
        "    return overallResult\n",
        "    \n",
        "def predictFromModel(model, tokeniser, data):\n",
        "    tf_batch = tokeniser(data, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
        "    tf_outputs = model(tf_batch)\n",
        "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "\n",
        "    return tf_predictions[:,1]    \n",
        "    \n",
        "def polarityDetermination(score, threshold = 0.35):\n",
        "    if score > threshold:\n",
        "        return 'POS'\n",
        "\n",
        "    elif score < -1 * threshold:\n",
        "        return 'NEG'\n",
        "\n",
        "    else:\n",
        "        return 'NEU'\n",
        "    \n",
        "def understandLinks(list_of_links):\n",
        "    for i in list_of_links:\n",
        "        if isinstance(i, str):\n",
        "            print(i)\n",
        "            \n",
        "def definePolarity(score1, score2, threshold = 0.35):\n",
        "    # Either both scores 1 & 2 are above threshold or when 1 of them is above threshold while the other is above 0\n",
        "    if (score1 > threshold and score2 > threshold) or (score1 > 0 and score2 > threshold) or (score1 > threshold and score2 > 0):\n",
        "        return 'POS'\n",
        "\n",
        "    # Either both scores 1 & 2 are below threshold or when 1 of them is below threshold while the other is below 0\n",
        "    elif (score1 < -1 * threshold and score2 < -1 * threshold) or (score1 < 0 and score2 < -1 * threshold) or (score1 < -1 * threshold and score2 < 0):\n",
        "        return 'NEG'\n",
        "\n",
        "    # Both scores are in the neutral range\n",
        "    elif (score1 >= -1 * threshold and score1 <= threshold) and (score2 >= -1 * threshold and score2 <= threshold):\n",
        "        return 'NEU'\n",
        "        \n",
        "    else:\n",
        "        return 'UNKNOWN'\n",
        "        \n",
        "def defineSubjectivity(score1, score2, threshold = 0.5): # score 1 is model score while score 2 is textblob score\n",
        "    if (score1 > threshold and score2 > threshold) or (score1 > threshold and score2 == threshold):\n",
        "        return 'SUBJECTIVE'\n",
        "\n",
        "    elif (score1 < threshold and score2 < threshold) or (score1 < threshold and score2 == threshold):\n",
        "        return 'OBJECTIVE'\n",
        "        \n",
        "    else:\n",
        "        return 'UNKNOWN'      \n",
        "\n",
        "def getPolarityProportion(df):\n",
        "    positivePolarity = len(df[df['overall_polarity'] == 'POS'])\n",
        "    negativePolarity = len(df[df['overall_polarity'] == 'NEG'])\n",
        "    \n",
        "    return {'positive': positivePolarity/len(df), 'negative': negativePolarity/len(df)}\n",
        "    \n",
        "def getSubjectivityProportion(df):\n",
        "    subjecitve = len(df[df['overall_subjectivity'] == 'SUBJECTIVE'])\n",
        "    objecitve = len(df[df['overall_subjectivity'] == 'OBJECTIVE'])\n",
        "    \n",
        "    return {'subjecitve': subjecitve/len(df), 'objecitve': objecitve/len(df)}\n",
        "    \n",
        "def flagPotentialBias(df):\n",
        "    result = []\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        if row['overall_polarity'] == 'NEG' or row['overall_polarity'] == 'POS' :\n",
        "            result.append(1)\n",
        "        \n",
        "        else:\n",
        "            result.append(0)\n",
        "            \n",
        "    return result"
      ],
      "metadata": {
        "id": "FhoEE5sE8OtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYkxgFr6KtmI"
      },
      "source": [
        "## Read the data and present as a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQMojddoHlA4"
      },
      "outputs": [],
      "source": [
        "tweets_for_analysis = ['1522931750451617793', '1507922082683793408', '553553331671408641']\n",
        "\n",
        "for i in range(len(tweets_for_analysis)):\n",
        "    print(i, tweets_for_analysis[i])\n",
        "\n",
        "tweetOption = 0 #int(input(\"Please indicate the tweet to analyse.\")) - 1\n",
        "\n",
        "print(f'\\nChosen {tweets_for_analysis[tweetOption]}\\n')\n",
        "\n",
        "if tweetOption == 0 or tweetOption == 1:\n",
        "    conversationID = tweets_for_analysis[tweetOption]\n",
        "\n",
        "    query = '''\n",
        "        select * from comments_for_analysis where conversation_id = '%s'\n",
        "    '''% conversationID\n",
        "    \n",
        "    df = getData(query, uri)\n",
        "\n",
        "    parent = df['head_id'][0]\n",
        "\n",
        "    print('About the tweet:')\n",
        "    print(getSingleTweetInfo(conversationID, header)['data'][0]['text'])\n",
        "\n",
        "    print('Number of comments:', len(df))\n",
        "\n",
        "elif tweetOption == 2:\n",
        "    conversationID = tweets_for_analysis[tweetOption]\n",
        "\n",
        "    query = '''\n",
        "        select * from pheme_dataset_for_analysis where head_id = '%s'\n",
        "    '''% parent\n",
        "\n",
        "    df = getData(query, uri)\n",
        "\n",
        "    print('Number of comments:', len(df))\n",
        "\n",
        "# Create directory to save all results\n",
        "os.mkdir(tweets_for_analysis[tweetOption])\n",
        "\n",
        "df['url'] = df['comment'].apply(lambda x: getLinks(x))\n",
        "df['link_title'] = df['url'].apply(lambda x: getURLfromList(x))\n",
        "df.head()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d92GdbPhQc-B"
      },
      "outputs": [],
      "source": [
        "# with open(mainDirectory + 'database_uri.txt', 'r', encoding=\"utf8\") as f:\n",
        "#     uri = f.read()\n",
        "\n",
        "# socialMedia = ['Reddit', 'Twitter', 'PHEME Dataset']\n",
        "\n",
        "# for i in range(len(socialMedia)):\n",
        "#     print(i, socialMedia[i])\n",
        "\n",
        "# socialMediaOption = 1\n",
        "\n",
        "# print(f'\\nChosen {socialMedia[socialMediaOption]}\\n')\n",
        "\n",
        "# if socialMedia[socialMediaOption] == 'Reddit':\n",
        "#     parent = 'rmqevj'\n",
        "#     query = '''\n",
        "#         select * from reddit_posts_for_analysis where head_id = '%s'\n",
        "#     '''% parent\n",
        "#     df = getData(query, uri)\n",
        "    \n",
        "# elif socialMedia[socialMediaOption] == 'Twitter':\n",
        "#     # conversationID = '1522931750451617793'\n",
        "#     conversationID = '1507922082683793408'\n",
        "\n",
        "#     query = '''\n",
        "#         select * from comments_for_analysis where conversation_id = '%s'\n",
        "#     '''% conversationID\n",
        "    \n",
        "#     df = getData(query, uri)\n",
        "\n",
        "#     parent = df['head_id'][0]\n",
        "\n",
        "#     print('About the tweet:')\n",
        "#     print(getSingleTweetInfo(conversationID, header)['data'][0]['text'])\n",
        "\n",
        "#     print('Number of comments:', len(df))\n",
        "\n",
        "# elif socialMedia[socialMediaOption] == 'PHEME Dataset':\n",
        "#     parent = '553553331671408641'\n",
        "#     query = '''\n",
        "#         select * from pheme_dataset_for_analysis where head_id = '%s'\n",
        "#     '''% parent\n",
        "#     df = getData(query, uri)\n",
        "\n",
        "# df['url'] = df['comment'].apply(lambda x: getLinks(x))\n",
        "# df['link_title'] = df['url'].apply(lambda x: getURLfromList(x))\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the CSV files"
      ],
      "metadata": {
        "id": "0ZSUwSh_9jno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/twitter_data_1540359703586377729.csv')\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "sIfC7-hm9l5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/twitter_data_1643638127804796931.csv')\n",
        "print(df2)\n",
        "parent = df2['head_id'][0]"
      ],
      "metadata": {
        "id": "4pKPrYcV9xq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df2.drop(3, inplace=True)\n",
        "df2.reset_index(drop=True, inplace=True)\n",
        "print(df2)"
      ],
      "metadata": {
        "id": "HMXraUqO52pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv('/content/twitter_data_1644171721212149764.csv')\n",
        "print(df3)\n",
        "parent = df3['head_id'][0]"
      ],
      "metadata": {
        "id": "0HoNPfcY92Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/twitter_data_PHEME2.csv')\n",
        "print(df1)\n",
        "parent = 552802011733716992"
      ],
      "metadata": {
        "id": "8nfAZ6CGE-hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlGAaDqsKEcQ"
      },
      "source": [
        "## Construction of tree of comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SA8N4lmQdI4"
      },
      "outputs": [],
      "source": [
        "from transformers.models.deformable_detr.modeling_deformable_detr import DeformableDetrFrozenBatchNorm2d\n",
        "root = Node(parent)\n",
        "\n",
        "input_list = [] \n",
        "\n",
        "item_count = df3['reply_to'].value_counts().to_dict()\n",
        "\n",
        "for i in range(len(df3['id'].tolist())):\n",
        "    try:\n",
        "      if DeformableDetrFrozenBatchNorm2d['id'].loc[i] != df3['reply_to'].loc[i]:\n",
        "          input_list.append((df3['id'].loc[i], df3['reply_to'].loc[i]))\n",
        "\n",
        "    except:\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEzZFu5LQdGK"
      },
      "outputs": [],
      "source": [
        "output_dict = make_map(input_list)\n",
        "output_dict\n",
        "createTweetsTree(output_dict, root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHxqfFO6myiX"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WnKFzsXi0kf"
      },
      "outputs": [],
      "source": [
        "pred_sentences = cleanComments(df3['comment'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Eaia1gVf-kZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3nUeAKC8fLZ"
      },
      "outputs": [],
      "source": [
        "main_directory = '/content/drive/MyDrive/FYP/Analyser/'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(main_directory + \"subjectivity_tokenizer/\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(main_directory + \"saved_subjectivity_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYR5nNqp82GB"
      },
      "outputs": [],
      "source": [
        "textblob_polarity = []\n",
        "textblob_subjectivity = []\n",
        "vader_results = []\n",
        "vaderCompoundScores = []\n",
        "model_subjectivity_result = predictFromModel(model, tokenizer, pred_sentences)\n",
        "\n",
        "for i in pred_sentences:\n",
        "  result = getSentimentalResults(i)\n",
        "  textblob_polarity.append(result['textblob_polarity'])\n",
        "  textblob_subjectivity.append(result['textblob_subjectivity'])\n",
        "  vader_results.append(result['vader_results'])\n",
        "  vaderCompoundScores.append(result['vader_compound_scores'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateBias(dataset):\n",
        "    count_positive_polarity_supportive = 0\n",
        "    count_negative_polarity_supportive = 0\n",
        "    count_positive_polarity_unsupportive = 0\n",
        "    count_negative_polarity_unsupportive = 0\n",
        "\n",
        "    for i in dataset.index.tolist():\n",
        "        if dataset['vader_compound_score'].loc[i] > 0.35 and dataset['topic_cluster'].loc[i] == 1:\n",
        "            count_positive_polarity_supportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] < -0.35 and dataset['topic_cluster'].loc[i] == 1:\n",
        "            count_negative_polarity_supportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] > 0.35 and dataset['topic_cluster'].loc[i] == 0:\n",
        "            count_positive_polarity_unsupportive += 1\n",
        "\n",
        "        elif dataset['vader_compound_score'].loc[i] < -0.35 and dataset['topic_cluster'].loc[i] == 0:\n",
        "            count_negative_polarity_unsupportive += 1\n",
        "            \n",
        "    total = count_positive_polarity_supportive + count_negative_polarity_supportive + count_positive_polarity_unsupportive + count_negative_polarity_unsupportive\n",
        "    \n",
        "    prob_D = (count_positive_polarity_supportive + count_negative_polarity_supportive)/total\n",
        "    prob_D_prime = (count_positive_polarity_unsupportive + count_negative_polarity_unsupportive)/total\n",
        "    result = {'P(D)': prob_D, 'P(D_p)': prob_D_prime}\n",
        "\n",
        "    try:\n",
        "        prob_D_H = count_positive_polarity_supportive / (count_positive_polarity_supportive + count_positive_polarity_unsupportive)\n",
        "        prob_D_Hprime = count_negative_polarity_supportive / (count_negative_polarity_supportive + count_negative_polarity_unsupportive)\n",
        "        \n",
        "        if prob_D_H/prob_D_Hprime > 1:\n",
        "            final_result = 1 / (prob_D_H/prob_D_Hprime)\n",
        "            \n",
        "        else:\n",
        "            final_result = prob_D_H/prob_D_Hprime\n",
        "\n",
        "        # return prob_D_H, prob_D_Hprime, final_result\n",
        "        return final_result\n",
        "\n",
        "    except:\n",
        "        prob_D_H  = 0\n",
        "        prob_D_Hprime = 0\n",
        "\n",
        "        # return prob_D_H, prob_D_Hprime, 1\n",
        "        return 1"
      ],
      "metadata": {
        "id": "nyfpzV7Scp3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDMlo4uJKpCo"
      },
      "source": [
        "## Saving of results from sentiment analysis into the same dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Yu1TVUrVA-"
      },
      "outputs": [],
      "source": [
        "#df2['number_of_links'] = df2['link_title'].apply(lambda x: len(x))\n",
        "\n",
        "# Polarity\n",
        "df3['textblob_polarity'] = textblob_polarity\n",
        "df3['vader_compound_score'] = vaderCompoundScores\n",
        "df3['vader_polarity'] = df3['vader_compound_score'].apply(lambda x: polarityDetermination(x))\n",
        "\n",
        "overall_polarity = []\n",
        "overall_polarity_scores = {}\n",
        "\n",
        "for i in range(len(textblob_polarity)):\n",
        "  overall_polarity.append(definePolarity(textblob_polarity[i], vaderCompoundScores[i]))\n",
        "  overall_polarity_scores[df3['id'][i]] = [textblob_polarity[i], vaderCompoundScores[i]]\n",
        "\n",
        "df3['overall_polarity'] = overall_polarity\n",
        "\n",
        "# Subjectivity\n",
        "df3['model_subjectivity'] = model_subjectivity_result\n",
        "df3['textblob_subjectivity'] = textblob_subjectivity\n",
        "\n",
        "overall_subjectivity = []\n",
        "overall_subjectivity_scores = {}\n",
        "\n",
        "for i in range(len(model_subjectivity_result)):\n",
        "  overall_subjectivity.append(defineSubjectivity(model_subjectivity_result[i], textblob_subjectivity[i]))\n",
        "  overall_subjectivity_scores[df3['id'][i]] = [float(model_subjectivity_result[i]), textblob_subjectivity[i]]\n",
        "\n",
        "df3['overall_subjectivity'] = overall_subjectivity\n",
        "\n",
        "df3['vader_sentiment'] = vader_results\n",
        "df3['topic_cluster'] = getClusters(pred_sentences, embedder)\n",
        "\n",
        "df3['potential_bias'] = flagPotentialBias(df3)\n",
        "\n",
        "#df2.to_csv('sentiment_result_kh.csv', index=False)\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculateBias(df3))"
      ],
      "metadata": {
        "id": "AmwbymlOcxH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938LTWg9Jx4n"
      },
      "source": [
        "## Results from sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['comment', 'topic_cluster']]"
      ],
      "metadata": {
        "id": "gce7H_wE_TLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0yrFfQevFHA"
      },
      "outputs": [],
      "source": [
        "df3[['id','comment','textblob_polarity','vader_compound_score','vader_polarity','model_subjectivity','textblob_subjectivity','topic_cluster','potential_bias']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjiwXxehtLZ6"
      },
      "source": [
        "# Confirmation Bias Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traceConversation(dataframe, tree, node, printGraphOption = True):\n",
        "    children_nodes_list = getAllChildNodes(tree, node, [])\n",
        "\n",
        "    print('\\n\\n')\n",
        "    new = search.find_by_attr(tree, node)\n",
        "    \n",
        "    if printGraphOption:\n",
        "        printGraph(new)\n",
        "\n",
        "    return dataframe[(dataframe['reply_to'].isin(children_nodes_list)) | (dataframe['id'].isin(children_nodes_list + [node]))], new"
      ],
      "metadata": {
        "id": "qDGGpgbL4GPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(parent)"
      ],
      "metadata": {
        "id": "K4kdqJAL4Lyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx9LO_Sksy4b"
      },
      "outputs": [],
      "source": [
        "head_thread = parent #input('Enter a comment to look at the replies. ')\n",
        "conversationDF, conversationTree = traceConversation(df2, root, head_thread, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm4w-2t68dr9"
      },
      "outputs": [],
      "source": [
        "print(\"About the tweet\\n\")\n",
        "\n",
        "print(\"Existence of links:\")\n",
        "checkLink = False\n",
        "\n",
        "for i in df['link_title']:\n",
        "    if isinstance(i, list):\n",
        "        print(i)\n",
        "        checkLink = False\n",
        "\n",
        "if checkLink:\n",
        "    print(\"No links\")\n",
        "\n",
        "else:\n",
        "  print(\"\\nThe exact link in the comments:\")\n",
        "  for i in df['url']:\n",
        "      if len(i) > 0:\n",
        "          print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4oHM11UC2Jk"
      },
      "outputs": [],
      "source": [
        "print(\"Conversation Tree\")\n",
        "printGraph(root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyx4dzByK3i9"
      },
      "source": [
        "## Results of confirmation bias analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serfbCYKuKrC"
      },
      "outputs": [],
      "source": [
        "print(\"Confirmation Bias Score for Entire Conversation:\", calculateBias(conversationDF))\n",
        "print(\"Number of potentially bias comments:\", len(conversationDF[conversationDF['potential_bias'] == 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fG2F2lCK7ia"
      },
      "source": [
        "## List of potentially bias comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKr3bkhm2sOz"
      },
      "outputs": [],
      "source": [
        "for index, row in conversationDF[conversationDF['potential_bias'] == 1].iterrows():\n",
        "    print(row['id'], row['comment'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BGK3AGIY8LUO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}